---
layout:     post
title:      深度学习入门学习笔记（零）
subtitle:   概念及算法总结
date:       2020-02-26
author:     aK
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - deep learn
    - conception
    - algorithm
---

>深度学习入门学习笔记（零）—— 概念及算法总结


# 1、 Python基础函数

## 1.1 numpy相关


导入numpy

	>>>import Numpy as np #将Numpy库命名为np导入

定义N维数组

	>>>A=np.array([[1,2,3],[4,5,6]])
	>>>print(A)
	[[1 2 3]
	 [4 5 6]]

数组形状等数据


	>>>print(A.shape)
	(2,3)
	>>>print(A.dtype)

数组维数、形状：

	>>>B=np.array([[1,2],[3,4],[5,6]])
	>>>print(b)
	[[1,2]
	 [3,4]
	 [5,6]]
	>>>np.ndim(B)
	2
	>>>B.shape
	(3,2)

数组的加减乘除为数组对应位置的加减乘除，或广播成对应位置的加减乘除

数组的点成为dot

	>>>np.dot(A,B)

访问数组某一部分元素

	>>> A=np.array([[1,2],[3,4]])
	>>> print(A)
	[[1 2]
	 [3 4]]
	>>> print(A[0])
	[1 2]
	>>> print(A[0][1])
	2


数组格式转换

	>>> B=A.flatten() #将数组转化成一维
	>>> print(B)
	[1 2 3 4]

或

	>>> a=np.array([[1,2,3],[4,5,6]])
	>>> np.reshape(a,6)
	array([1, 2, 3, 4, 5, 6])
	>>> np.reshape(a,(2,3))
	array([[1, 2, 3],
     	  [4, 5, 6]])


选出大于某个数

	>>> print(B>2) #选出B中大于2的数，以布尔数值的形式表示
	[False False  True  True]
	>>> print(B[B>2]) #写出B中大于2的数
	[3 4]

建立随机数组

	>>> from numpy import random as nr
	>>> r=nr.randint(0,10,size=(4,3))
	>>> r
	array([[2, 3, 1],
  	      [6, 7, 1],
   	      [9, 2, 3],
    	  [9, 7, 5]])

## 1.2 matplotlib


在同一张图上绘制多条曲线

	import numpy as np
	import matplotlib.pyplot as plt

	#生成数据
	x=np.arange(0,6,0.1)
	y1=np.sin(x)
	y2=np.cos(x)

	#绘制图像
	plt.plot(x,y1,label='sin')
	plt.plot(x,y2,linestyle='--',label='cos')
	plt.xlabel('x')
	plt.ylabel('y')
	plt.title('sin & cos')
	plt.legend
	plt.show()

读取本地图像并显示

	import numpy as np
	import matplotlib.image as imread
	img=imread('lene.png')
	plt.imshow(img)
	plt.show()


# 2、 名词解释

* 感知机

感知机（perceptron）是由美国学者Frank Rosenblatt在1957年提出的。感知机是神经网络（深度学习）的起源算法



# 3、 基本函数


## 3.1 激活函数（activation function）

* 阶跃函数

阶跃函数表达式为：

	y=h（b+w1x1+w2x2）
	h(x)=0 x<=0
	h(x)=1 x>0

在python中表示为：

	import numpy as np
	def step_function(x):
        return np.array(x > 0, dtype=np.int) #dtype=np.int的意思是将布尔函数转换为0，1

* sigmoid函数

sigmoid函数表达式为：

	h(x)=1/(1+exp(-x))

sigmoid函数在python中的代码为：

	def sigmoid(x):
        return 1 / (1 + np.exp(-x)) 

* ReLU函数

ReLU函数表达式为：
	
	h(x)=x x>0
	h(x)=0 x<=0

其Python函数为：

	def relu(x):
        return np.maximum(0, x)


输出层的前一层到输出层的激活函数为恒等函数，即不作变化

在进行分类问题时，可以使用softmax函数进行处理。

	def softmax(x):
        if x.ndim == 2:
            x = x.T
            x = x - np.max(x, axis=0)
            y = np.exp(x) / np.sum(np.exp(x), axis=0)
            return y.T 

        x = x - np.max(x) # 溢出对策
        return np.exp(x) / np.sum(np.exp(x))

# 4、高级函数

## 4.1 从数据中学习

在实际的神经网络中，权重参数极多，人工决定这些参数的值是不现实的，因次需要通过学习数据来推出这些权重参数的值。

机器学习中，一般将数据分为 **训练数据** 和 **测试数据** 两部分，训练数据也可被成为 **监督数据** 。

**泛化能力** 是指处理未被观察过的数据（不包含在训练数据中的数据）。 **获得泛化能力** 是机器学习的最终目标。

仅仅用一个数据集去学习和评价权重参数，是无法正确评价，会导致可以顺利的处理某个数据集，而无法处理其他数据集。

只对某个数据集拟合度极高的状态称为 **过拟合（over fitting）** 。避免过拟合也是机器学习的重要课题。

## 4.2 损失函数

神经网络的学习通过某个指标表示现在的状态。然后，以这个指标为基准，寻找最优权重参数。神经网络的学习使用的指标为 **损失函数（loss function）** 。这个损失函数一般使用均方误差和交叉熵误差。

* 均方误差

均方误差在python的表示为：

	def mean_squared_error(y, t):
        return 0.5 * np.sum((y-t)**2)

* 交叉熵误差

交叉熵误差在python的表示为：

	def cross_entropy_error(y, t):
        delta=1e-7
	    return -np.sum(t*np.lpg(y+delta))

交叉熵函数实质上是正确解标签的输出概率的自然对数。也就是说，交叉熵误差的值是由正确解标签所对应的输出概率决定的。

对应的mini-batch的交叉熵误差(标签为独热数据)再python中的表示为：

	def cross_entropy_error(y, t):
        if y.ndim == 1:
            t = t.reshape(1, t.size)
            y = y.reshape(1, y.size)
        
        # 监督数据是one-hot-vector的情况下，转换为正确解标签的索引
        if t.size == y.size:
            t = t.argmax(axis=1)
             
        batch_size = y.shape[0]
        return -np.sum(np.log(y[np.arange(batch_size), t] + 1e-7)) / batch_size

**损失函数的意义** 是在寻找最优参数（权重和偏置）时，要寻找使损失函数最小的参数。为了找到使损失函数最小的参数，需要引入损失函数关于参数的导数（梯度），然后以导数为指引，逐步更新参数的值。

>在神经网络进行学习时，不能将识别精度作为指标，因为如果以识别精度作为指标，则参数的导数在绝大多数的地方都会变为0.


## 4.4 梯度

梯度定义见高等数学。

其定义在python中的表达为：

	def numerical_gradient(f, x):
    	h = 1e-4 # 0.0001
    	grad = np.zeros_like(x)
    
    	it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])
    	while not it.finished:
        	idx = it.multi_index
        	tmp_val = x[idx]
        	x[idx] = float(tmp_val) + h
        	fxh1 = f(x) # f(x+h)
        
       		x[idx] = tmp_val - h 
        	fxh2 = f(x) # f(x-h)
        	grad[idx] = (fxh1 - fxh2) / (2*h)
        
        	x[idx] = tmp_val # 还原值
        	it.iternext()   
        
    	return grad   

**梯度法（gradient methend）** ：函数的取值从当前位置沿着梯度方向前进一定距离，然后在新的位置重新求取梯度，再沿着新的梯度方向前进，如此反复，不断沿着新梯度的方向前进，这种通过沿梯度前进从而减小函数值的方法就是梯度法。

梯度法时解决机器学习中最优化问题的常见方法，特别是在神经网络的学习中经常被用到。

在python中实现梯度法的表达为：

	def gradient_descent(f, init_x, lr=0.01, step_num=100):
    	x = init_x
    	x_history = []

    	for i in range(step_num):
       		x_history.append( x.copy() )

        	grad = numerical_gradient(f, x)
        	x -= lr * grad

    	return x, np.array(x_history)

## 4.5 学习算法的实现

神经网络的基础知识即位上述内容。通过损失函数、mini-batch、梯度、梯度下降法等内容，即可实现神经网络的学习。神经网络的学习步骤如下：

**步骤1 mini-batch**

从训练数据中随机挑选一部分数据，这部分数据称为mini-batch。我们的目标是减少mini-batch的损失函数的值

**步骤2 计算梯度**

为了减小mini-batch的损失函数值，需要求出各个权重参数的梯度，梯度表示损失函数的值减小最多的方向。


**步骤3 更新权重**

将权重参数沿梯度最小的方向进行微小更新。

**步骤4 重复步骤1-3**