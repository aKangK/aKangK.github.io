---
layout:     post
title:      深度学习入门学习笔记（五）
subtitle:   利用反向传播，加快计算参数关于损失函数的梯度，提高神经网络学习的速度
date:       2020-02-28
author:     aK
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - Python
    - Neural Networks
    - fitting
---

>深度学习入门学习笔记（五）


# 第五章 误差反向传播法

误差反向传播法是一种能高效计算权重参数梯度的方法。

## 5.1 简单层的实现

乘法节点称为 **“乘法层”（MulLayer）** ，加法节点称为 **“加法层”（AddLayer）**

对于乘法层，其在python中的实现如下：

	class MulLayer:
    	def __init__(self):
        	self.x = None
        	self.y = None
		
		#正向传播
    	def forward(self, x, y):
        	self.x = x
        	self.y = y                
        	out = x * y

        	return out

    	#反向传播
		def backward(self, dout):
        	dx = dout * self.y
        	dy = dout * self.x

        	return dx, dy

对于加法层，其在python中的实现如下：

	class AddLayer:
    	def __init__(self):
        	pass

		#正向传播
    	def forward(self, x, y):
        	out = x + y

        	return out

    	#反向传播
    	def backward(self, dout):
        	dx = dout * 1
        	dy = dout * 1

        	return dx, dy







## 5.2 激活函数层的实现

激活函数ReLU的定义为：

	y=x   x>0
	y=0   x<=0

则其导数为：

	y=1    x>0
	y=0    x<=0

ReLU层在python中的表示为：

	class Relu:
    	def __init__(self):
        	self.mask = None

    	def forward(self, x):
        	self.mask = (x <= 0)
        	out = x.copy()
        	out[self.mask] = 0

        	return out

    	def backward(self, dout):
        	dout[self.mask] = 0
        	dx = dout

        	return dx

同理，sigmoid层在python中的表示为：

	class Sigmoid:
    	def __init__(self):
        	self.out = None

    	def forward(self, x):
        	out = sigmoid(x)
        	self.out = out
        	return out

    	def backward(self, dout):
        	dx = dout * (1.0 - self.out) * self.out

        	return dx

## 5.3 Affine/Softmax层的实现

批处理的Affine层在python中的表示为：

	class Affine:
    	def __init__(self, W, b):
        	self.W =W
        	self.b = b
        
        	self.x = None
        	self.original_x_shape = None
        	# 权重和偏置参数的导数
        	self.dW = None
        	self.db = None

    	def forward(self, x):
        	# 对应张量
        	self.original_x_shape = x.shape
        	x = x.reshape(x.shape[0], -1)
        	self.x = x

        	out = np.dot(self.x, self.W) + self.b

        	return out

    	def backward(self, dout):
        	dx = np.dot(dout, self.W.T)
        	self.dW = np.dot(self.x.T, dout)
        	self.db = np.sum(dout, axis=0)
        
        	dx = dx.reshape(*self.original_x_shape)  # 还原输入数据的形状（对应张量）

softmax-with-loss层在python中的表示为：

	class SoftmaxWithLoss:
    	def __init__(self):
        	self.loss = None
        	self.y = None # softmax的输出
        	self.t = None # 监督数据

    	def forward(self, x, t):
        	self.t = t
        	self.y = softmax(x)
        	self.loss = cross_entropy_error(self.y, self.t)
        
        	return self.loss

    	def backward(self, dout=1):
        	batch_size = self.t.shape[0]
        	if self.t.size == self.y.size: # 监督数据是one-hot-vector的情况
            	dx = (self.y - self.t) / batch_size
        	else:
            	dx = self.y.copy()
            	dx[np.arange(batch_size), self.t] -= 1
            	dx = dx / batch_size
        
        	return dx


## 5.4 误差反向传播法的实现

	# coding: utf-8
	import sys, os
	sys.path.append(os.pardir)

	import numpy as np
	from dataset.mnist import load_mnist
	from two_layer_net import TwoLayerNet

	# 读入数据
	(x_train, t_train), (x_test, t_test) = load_mnist(normalize=True, one_hot_label=True)

	network = TwoLayerNet(input_size=784, hidden_size=50, output_size=10)

	iters_num = 10000
	train_size = x_train.shape[0]
	batch_size = 100
	learning_rate = 0.1

	train_loss_list = []
	train_acc_list = []
	test_acc_list = []

	iter_per_epoch = max(train_size / batch_size, 1)

	for i in range(iters_num):
    	batch_mask = np.random.choice(train_size, batch_size)
    	x_batch = x_train[batch_mask]
    	t_batch = t_train[batch_mask]
    
    	# 梯度
    	#grad = network.numerical_gradient(x_batch, t_batch)
    	grad = network.gradient(x_batch, t_batch)
    
    	# 更新
    	for key in ('W1', 'b1', 'W2', 'b2'):
        	network.params[key] -= learning_rate * grad[key]
    
    	loss = network.loss(x_batch, t_batch)
    	train_loss_list.append(loss)
    
    	if i % iter_per_epoch == 0:
        	train_acc = network.accuracy(x_train, t_train)
        	test_acc = network.accuracy(x_test, t_test)
        	train_acc_list.append(train_acc)
        	test_acc_list.append(test_acc)
        	print(train_acc, test_acc)





