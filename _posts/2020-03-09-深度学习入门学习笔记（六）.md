---
layout:     post
title:      深度学习入门学习笔记（六）
subtitle:   寻找最优权重参数的最优化方法、权重参数的初始值、超参数的设定方法、权值衰减、Dropout、Batch Normalization
date:       2020-02-28
author:     aK
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - Python
    - Neural Networks
    - parameter
---

>深度学习入门学习笔记（六）


# 第六章 与学习相关的技巧

本章介绍了神经网络的学习中的一些重要的观点。

## 6.1 参数的更新

### 6.1.1 随机梯度下降法

神经网络的学习的目的是找到使损失函数尽可能小的参数。这是寻找最优化的参数的问题，解决这个问题的过程称为**最优化（optimization）** 。

前几章中使用的方法称为**随机梯度下降法（stochastic gradient descent，SGD）**

SGD的缺点在于，如果函数的形状**非均向（anisotropic）**，比如呈延申状，搜索路径的效率就会非常低，这是因为梯度的方向是指向梯度下降最快的方向，而非最小值的方向。

### 6.1.2 Momentum

Momentum法的数学式为：

	v=av-n(ΔL/ΔW)
	W=W+v

各参数的含义与SGD相同，新增的v表示物理上的速度，a为加速度，Momentum给人的感觉是小球在地面上滚动，收到摩擦力的影响，导致速度越来越小。

Momentum法在python中的表示为：

	class Momentum:

    	"""Momentum SGD"""

    	def __init__(self, lr=0.01, momentum=0.9):
        	self.lr = lr
        	self.momentum = momentum
        	self.v = None
        
    	def update(self, params, grads):
        	if self.v is None:
            	self.v = {}
            	for key, val in params.items():                                
                	self.v[key] = np.zeros_like(val)
                
        	for key in params.keys():
            	self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] 
            	params[key] += self.v[key]

### 6.1.3 AdaGrad

AdaGrad方法会为参数的每个元素适当的调整学习率，于此同时进行学习。

	class AdaGrad:

    	"""AdaGrad"""

    	def __init__(self, lr=0.01):
        	self.lr = lr
        	self.h = None
        
    	def update(self, params, grads):
        	if self.h is None:
            	self.h = {}
            	for key, val in params.items():
                	self.h[key] = np.zeros_like(val)
            
        	for key in params.keys():
            	self.h[key] += grads[key] * grads[key]
            	params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)

### 6.1.4 Adam

Momentum参照小球在碗里滚动的物理规则进行移动，AdaGrad为参数的每个元素适当的调整学习率。将两个方法融合，即为Adam法

	class Adam:

    	"""Adam (http://arxiv.org/abs/1412.6980v8)"""

    	def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):
        	self.lr = lr
        	self.beta1 = beta1
        	self.beta2 = beta2
        	self.iter = 0
        	self.m = None
        	self.v = None
        
    	def update(self, params, grads):
        	if self.m is None:
            	self.m, self.v = {}, {}
            	for key, val in params.items():
                	self.m[key] = np.zeros_like(val)
                	self.v[key] = np.zeros_like(val)
        
        	self.iter += 1
        	lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         
        
        	for key in params.keys():
            	#self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]
            	#self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)
            	self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])
            	self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])
            
            	params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)
            
            	#unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias
            	#unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias
            	#params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)

与SGD相比，其他3种方法学习速度更快，精度更高，其中，AdaGrad法的速度略高于其他两种方法。 

## 6.2 权重的初始值

之前的初始权重值为0.01*np.random.randn(10,100),即标准差为0.01的高斯分布。

为了抑制过拟合，提高泛化能力，可以使用**权值衰减（weight decay）** 的方法，简单来说，就是一种以减小权重参数的值为目的进行学习的方法。

如果想减小权重参数的值，可否在最初将权重参数设置为0呢？

不可以，原因是在误差反向传播法中，所有的权重值都会进行相同的更新，如果初始权重参数设置为0，则反向传播的权重都会进行相同的更新，并拥有了对称的值（重复的值），这使得神经网络拥有许多不同的权重的意义丧失了。为了防止“权重均一化”，必须随机生成初始权重。

在Xavier的论文中，为了各层的激活值呈现出具有相同广度的分布，推到了合适的权重尺度，结论是  




## 6.3 Batch Normalization






## 6.4 正则化





## 6.5 超参数的验证






