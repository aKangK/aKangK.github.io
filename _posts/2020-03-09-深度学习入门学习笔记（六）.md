---
layout:     post
title:      深度学习入门学习笔记（六）
subtitle:   寻找最优权重参数的最优化方法、权重参数的初始值、超参数的设定方法、权值衰减、Dropout、Batch Normalization
date:       2020-02-28
author:     aK
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - Python
    - Neural Networks
    - parameter
---

>深度学习入门学习笔记（六）


# 第六章 与学习相关的技巧

本章介绍了神经网络的学习中的一些重要的观点。

## 6.1 参数的更新

### 6.1.1 随机梯度下降法

神经网络的学习的目的是找到使损失函数尽可能小的参数。这是寻找最优化的参数的问题，解决这个问题的过程称为**最优化（optimization）** 。

前几章中使用的方法称为**随机梯度下降法（stochastic gradient descent，SGD）**

SGD的缺点在于，如果函数的形状**非均向（anisotropic）**，比如呈延申状，搜索路径的效率就会非常低，这是因为梯度的方向是指向梯度下降最快的方向，而非最小值的方向。

### 6.1.2 Momentum

Momentum法的数学式为：

	v=av-n(ΔL/ΔW)
	W=W+v

各参数的含义与SGD相同，新增的v表示物理上的速度，a为加速度，Momentum给人的感觉是小球在地面上滚动，收到摩擦力的影响，导致速度越来越小。

Momentum法在python中的表示为：

	class Momentum:

    	"""Momentum SGD"""

    	def __init__(self, lr=0.01, momentum=0.9):
        	self.lr = lr
        	self.momentum = momentum
        	self.v = None
        
    	def update(self, params, grads):
        	if self.v is None:
            	self.v = {}
            	for key, val in params.items():                                
                	self.v[key] = np.zeros_like(val)
                
        	for key in params.keys():
            	self.v[key] = self.momentum*self.v[key] - self.lr*grads[key] 
            	params[key] += self.v[key]

### 6.1.3 AdaGrad

AdaGrad方法会为参数的每个元素适当的调整学习率，于此同时进行学习。

	class AdaGrad:

    	"""AdaGrad"""

    	def __init__(self, lr=0.01):
        	self.lr = lr
        	self.h = None
        
    	def update(self, params, grads):
        	if self.h is None:
            	self.h = {}
            	for key, val in params.items():
                	self.h[key] = np.zeros_like(val)
            
        	for key in params.keys():
            	self.h[key] += grads[key] * grads[key]
            	params[key] -= self.lr * grads[key] / (np.sqrt(self.h[key]) + 1e-7)

### 6.1.4 Adam

Momentum参照小球在碗里滚动的物理规则进行移动，AdaGrad为参数的每个元素适当的调整学习率。将两个方法融合，即为Adam法

	class Adam:

    	"""Adam (http://arxiv.org/abs/1412.6980v8)"""

    	def __init__(self, lr=0.001, beta1=0.9, beta2=0.999):
        	self.lr = lr
        	self.beta1 = beta1
        	self.beta2 = beta2
        	self.iter = 0
        	self.m = None
        	self.v = None
        
    	def update(self, params, grads):
        	if self.m is None:
            	self.m, self.v = {}, {}
            	for key, val in params.items():
                	self.m[key] = np.zeros_like(val)
                	self.v[key] = np.zeros_like(val)
        
        	self.iter += 1
        	lr_t  = self.lr * np.sqrt(1.0 - self.beta2**self.iter) / (1.0 - self.beta1**self.iter)         
        
        	for key in params.keys():
            	#self.m[key] = self.beta1*self.m[key] + (1-self.beta1)*grads[key]
            	#self.v[key] = self.beta2*self.v[key] + (1-self.beta2)*(grads[key]**2)
            	self.m[key] += (1 - self.beta1) * (grads[key] - self.m[key])
            	self.v[key] += (1 - self.beta2) * (grads[key]**2 - self.v[key])
            
            	params[key] -= lr_t * self.m[key] / (np.sqrt(self.v[key]) + 1e-7)
            
            	#unbias_m += (1 - self.beta1) * (grads[key] - self.m[key]) # correct bias
            	#unbisa_b += (1 - self.beta2) * (grads[key]*grads[key] - self.v[key]) # correct bias
            	#params[key] += self.lr * unbias_m / (np.sqrt(unbisa_b) + 1e-7)

与SGD相比，其他3种方法学习速度更快，精度更高，其中，AdaGrad法的速度略高于其他两种方法。 

## 6.2 权重的初始值

之前的初始权重值为0.01*np.random.randn(10,100),即标准差为0.01的高斯分布。

为了抑制过拟合，提高泛化能力，可以使用**权值衰减（weight decay）** 的方法，具体介绍在6.3中，简单来说，就是一种以减小权重参数的值为目的进行学习的方法。

如果想减小权重参数的值，可否在最初将权重参数设置为0呢？

不可以，原因是在误差反向传播法中，所有的权重值都会进行相同的更新，如果初始权重参数设置为0，则反向传播的权重都会进行相同的更新，并拥有了对称的值（重复的值），这使得神经网络拥有许多不同的权重的意义丧失了。为了防止“权重均一化”，必须随机生成初始权重。

在Xavier的论文中，为了各层的激活值呈现出具有相同广度的分布，推到了合适的权重尺度，结论是**对于sigmoid函数和tanh函数作为激活函数时，如果前一层的节点数为n，则初始值使用标准差为1/(n)^0.5的分布**，此即为**Xavier初始值**，在一般的深度学习框架中，Xavier初始值已被作为标准使用。比如，在Caffe框架中，通过在设定权重初始值时赋予Xavier参数，就可以使用Xavier初始值。

当激活函数为ReLU时，其专用的初始值为**He初始值**（Kaiming He），即**如果前一层的节点数为n，则初始值使用标准差为（1/n)^0.5的分布**

He初始值直观的解释为，相较于sigmoid函数和tanh函数具有左右对称性，ReLU函数负值区域的值为0，为了使其更具有广度，需要2倍的系数。



## 6.3 Batch Normalization

batch normalizati，顾名思义，就是在进行mini-batch学习时，进行正规化处理，使数据分布的均值为0，方差为1.公式如下：

![Batch Norm公式](https://note.youdao.com/yws/api/personal/file/504D185377874BF4A4DB0A4FCBD6A93C?method=download&shareKey=eaa57f1661b7a35894a9eae369012504)

因此，需要在神经网络层中加入normal batch层，即：

![Batch Norm例子](https://note.youdao.com/yws/api/personal/file/1546A74C275F4800A6A3E2A2171E034E?method=download&shareKey=c2cf52d1a4337d0b99e124985efccf27)

此外，还需要对正规化后的数据进行缩放和平移。

使用batch norm后，会加快学习速度，并使得初始权重值更加耐操，即不那么依赖初始值。


## 6.4 正则化

**过拟合**指的是只能拟合训练数据，但不能拟合包含在训练数据中的其他数据的状态。

**机器学习的目标**是提高泛化能力，即便是在没有包含在训练数据里的未观测数据，也希望模型能构进行正确的识别。因此，需要一定抑制“过拟合”的技巧。

### 6.4.1 过拟合

发生过拟合的原因主要有两个：

* 模型拥有大量参数，表现力强
* 训练数据少

### 6.4.2 权值衰减

**权重衰减**是一种常用的抑制过拟合的方法。该方法通过在学习过程中对大的权重进行惩罚，来抑制过拟合。一般是为损失函数加上群众的平方范数（L2范数）。

### 6.4.3 Dropout

权重衰减是一种常用的抑制过拟合的方法，但是在模型复杂时，权值衰减法效果较差，一般采取**Dropout法**。

**Dropout法**时一种在学习的过程中随机删除神经元的方法。训练时，随机选出隐藏层的神经元，然后将其删除。被删除的神经元不再进行信号的传递。训练时，每传递一次数据，就会随机选择要删除的神经元。然后，测试时，虽然会传递所有的神经元信号，但是对于各个神经元的输出，要乘上训练时的删除比例后再输出。如下图所示。

![Dropout法示意图](https://note.youdao.com/yws/api/personal/file/0A422D3058374C9BB942D916F8657DF7?method=download&shareKey=a508f29ef9b7ffa70604cd63a7bf27ff)



## 6.5 超参数的验证

神经网络中，除了权重和偏置等参数，还包含**超参数（hyper-parameter）**，如各层的神经元数量、batch大小、参数更新时的学习率或权值衰减等。如果这些参数没有设置合适的值，模型的性能会很差。

### 6.5.1 验证数据

之前数据划分为训练数据和测试数据，训练数据用于学习，测试数据用于评估泛化能力，由此就可以评估是否发生了过拟合。

再调整超参数的时候，必须使用超参数专用的确认数据，需要划分出验证数据。

### 6.5.2 超参数的最优化

进行超参数最优化选择时，需要先大致地指定范围，类似于10^-3至10^3这样。

第六章结束











