---
layout:     post
title:      深度学习入门学习笔记（七）
subtitle:   卷积神经网络介绍
date:       2020-04-15
author:     aK
header-img: img/post-bg-ios9-web.jpg
catalog: true
tags:
    - Python
    - Neural Networks
    - convolutional
---

>深度学习入门学习笔记（七）


# 第七章 卷积神经网络

本章介绍了卷积神经网络（Convolutional Neural Network, CNN）的结构。

## 7.1 整体结构

普通的神经网络中，相邻层的神经元之间全都有连接，这称为全连接（fully-connected），并利用Affine层实现了全连接层。而相较于之前的神经网络结构，卷积神经网络中出现了卷积层（convolution层）和池化层（pooling层），CNN网络的结构如下图。

![CNN网络结构示例](https://note.youdao.com/yws/api/personal/file/2E8CDA4D07A24017AD0798B4E9817D08?method=download&shareKey=28ef68ed427d6edf5c63322a4c592a53)

## 7.2 卷积层

### 7.2.1 全连接层存在的问题

普通的神经网络中使用了全连接层（Affine）层，在全连接层中，相邻层的神经元全部连接在一起，输出的数量可以任意确定，但存在一个问题，即数据的形状被忽视了。例如，输入数据为图像时，图像时长、宽和颜色的三位形状，但在全连接层中，需要将三维数据拉平为1维数据，这导致图像相邻像素的关系被消除了。

而卷积层可以保持形状不变，当输入数据为图像时，卷积层会以3维数据的形式接收输入数据，并同样以3维数据的形式将结果输出至下一层。因此，在CNN中，可以正确理解图像的形状的含义。

在CNN中，有时将卷积层的输入输出数据称为**特征图（feature map）**。其中，卷积层的输入数据称为**输入特征图（input feature map）**，输出数据称为**输出特征图（output feature map）**。


### 7.2.2 卷积运算

卷积层进行的处理就是卷积运算。卷积运算相当于图像处理中的“滤波器运算”。有些文献中，也会将“滤波器”称为“核”。

对于输入数据，卷积运算以一定的间隔华东滤波器的窗口并应用，然后将结果保存到输出的对应位置。将这个过程在所有的位置都进行一遍，就可以得到卷积运算的输出。

在CNN中，滤波器的参数即为权重，并且，CNN中也存在偏置，且偏置通常只有一个值。具体运算过程如下图所示。


![卷积运算的权重部分计算顺序](https://note.youdao.com/yws/api/personal/file/BF2B1FC8E9374327B64644921D4DCBFD?method=download&shareKey=0a7ffee9f0921d058939ea825ca40abe)

![卷积运算的偏置](https://note.youdao.com/yws/api/personal/file/4EA61E046E124CBF9EBBCC6F6B8F8E19?method=download&shareKey=8cb2f8a80fe90f8f73d8b111972c861b)



### 7.2.3 填充

在进行卷积层处理之前，有时需要在输入数据的周围填入固定的数据，例如0。幅度为1的填充，是指填充了一圈的0或其他任意整数。

因为每做一次卷积，都会将空间缩小，因此需要进行填充处理。


### 7.2.4 步幅

应用滤波器的位置间隔称为**步幅（stride）**


### 7.2.5 3维数据的卷积计算

在进行3维数据的卷积计算时，需要将滤波器的维度也设置为3，并将输出结果相加，最终得到1维数据。

### 7.2.6 结合方块思考

如果在进行3维数据卷积计算时，想要得到多个卷积输出，则需要多个3维的滤波器。

### 7.2.7 批处理

普通的神经网络可以进行批处理，在卷积神经网络中也可以进行批处理。

因此，需要在各层的传递的数据中增加一个维度，即batch_num维度，将维度更改为：（batch_num，channel，height，width），从而将数据作为4维的形状进行传递，一次性对N个数据进行卷积运算。

## 7.3 池化层

池化是缩小高、长方向上的空间的运算。一般的池化放式有Max池化Average池化。在图像识别领域，一般采用Max池化。

池化层没有要进行学习的参数。

池化层的通道数不发生变化

池化层对微小的位置变化具有“鲁棒性”，鲁棒性可以理解为**耐操性**，即输入数据发生微小变化时，池化层仍会返回相同的结果。


## 7.4 卷积层和池化层的实现


### 7.4.1 4维数组

对于多个普通的图像，可以理解维4维数组，即（批，通道数，高，宽）


### 7.4.2 基于im2col的展开

如果按照原理进行卷积计算，会用到多个for循环，因此可以使用im2col函数进行展开。

对于3维的数据应用im2col后，数据会转换维2维数据。

再同样应用im2col函数对滤波器进行转换，再将转换后的数据相乘。

使用im2col的实现存在比普通的实现消耗更多内存的缺点，但汇总成一个巨大的矩阵进行计算，可以提高计算效率。

![卷积运算的实现](https://note.youdao.com/yws/api/personal/file/F914975F4F5E44C0B00E5E75BBA9546A?method=download&shareKey=7449749c913c078ab4a77b7021f3556a)

### 7.4.3 卷积层的实现

卷积层的实现，除了使用im2col之外，反向传播与Affine层的实现相同。

### 7.4.4 池化层的实现

池化层的实现与卷积层一样，都使用im2col函数来实现，但不同的是，池化层在通道方向上是独立的。

具体的讲，池化的应用区域按通道单独展开，如下图所示。

![池化层的展开](https://note.youdao.com/yws/api/personal/file/0FD045484108436AA3495D2985F83A19?method=download&shareKey=6c35f807dbe497b71f1fe366d20a035f)

展开后，对于Max池化而言，只需要对展开的矩阵的各行求最大值即可，如下图。

![池化层的实现过程](https://note.youdao.com/yws/api/personal/file/F3EAC7E424A24DFEBD895858456A29C8?method=download&shareKey=b91a4333df7c6ee32a9ae7a3b65a756c)

## 7.5 CNN的实现

基于上述学习，可以实现CNN，具体过程可参见《深度学习入门：基于Python的理论与实现》及其源代码

## 7.6 CNN的可视化

目前比较主流的观点是，CNN的滤波器在观察边缘、斑块等内容，且随着层数的加深，提取的信息越来越抽象，难以被人所理解。


## 7.7 具有代表性的CNN

### 7.7.1 LeNet

CNN的元祖LeNet在1998年被提出，应用于手写数字是被的网络。

与现在产常见的CNN相比，LeNet有几个不同点，第一个不同点在于激活函数，现在常用的激活函数维ReLU，而LeNet采用的是sigmoid函数；现在的池化方式为Max，而LeNet为子采样（subsampling）缩小中间数据的大小。

### 7.7.2 AlexNet

在LeNet提出的20多年后，AlexNet面试。AlexNet是引发深度学习热炒的导火索，不过它的网络结构与LeNet几乎相同，区别在于：

* 激活函数使用了ReLU
* 使用局部正规化的LRN（local response normalization）层
* 使用了Dropout（6.4.3节）




